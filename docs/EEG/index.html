
<!DOCTYPE html>
<html>

<head lang="en">
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B4PERRK1L1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-B4PERRK1L1');
</script>
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>On the Feasibility of EEG-based Motor Intention Detection for Real-Time Robot Assistive Control</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://jonbarron.info/zipnerf/img/nottingham.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://jonbarron.info/zipnerf/"/>
    <meta property="og:title" content="Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields" />
    <meta property="og:description" content="Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8%-77% lower than either prior technique, and that trains 24x faster than mip-NeRF 360." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields" />
    <meta name="twitter:description" content="Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8%-77% lower than either prior technique, and that trains 24x faster than mip-NeRF 360." />
    <meta name="twitter:image" content="https://jonbarron.info/zipnerf/img/teaser.jpg" />


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚡</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>On the Feasibility of EEG-based Motor Intention Detection<br> for Real-Time Robot Assistive Control</b></br> 
                <small>
                ICRA 2023 (Under review)
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                          Ho Jin Choi*
                    </li>
                    <li>
                        Satyajeet Das*
                    </li>
                    <li>
                        <a href="http://dorverbin.github.io/">
                          Shaoting Peng*
                        </a>
                    </li>
                    <li>
                        <a href="https://www.grasp.upenn.edu/people/ruzena-bajcsy/">
                          Ruzena Bajcsy
                        </a>
                    </li>
                    <li>
                        <a href="https://nbfigueroa.github.io/">
                          Nadia Figueroa
                        </a>
                    </li>
                    </br>University of Pennsylvania
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="../data/EEG.pdf">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://www.youtube.com/watch?v=NFpO8_4gKWQ&t=1s">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


<!-- 
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/teaser.mp4" type="video/mp4" />
                </video>
						</div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    This paper investigates the feasibility of using
EEG-based intention detection for real-time robot assistive
control, with a focus on motor intention prediction. The
proposed approach involves two pipelines: i) an offline pipeline
that collects and processes EEG data as well as motion data to
train a classifier for motion intention prediction and biological
interpretation, and ii) an online pipeline that uses the trained
classifier to predict a human’s motor intention and couples it
with a robot to perform assistive control. We adopt and modify
the state-of-the-art EEG sample covariance matrix feature
representation by using EEG signal derivatives and tangent
space projection as features for an SVM classifier that can
run in real-time. With this, Our system excels with the highest
accuracy of 86.88% on recorded testing data, and it achieves an
impressive 70% accuracy in real robot experiments. We show
in a real-robot experiment that our online pipeline is able to
detect the onset of motion purely from EEG signals and trigger
a robot to perform an assistive task.
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/NFpO8_4gKWQ?si=TA09uVeVBPTmwcT5" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>
<br>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>
				<table style="width: 100%; border-collapse: collapse;">
				  <tr>
				    <td style="text-align: center;">
		                <!-- <div class="one"> -->
                            <img src='img/pipeline.png' width="700">
                          <!-- </div> -->
					</td>
				  </tr>
				</table>
                <p class="text-justify">
                    Our offline and online pipeline: The upper part is the offline pipeline, which first gathers the raw EEG data and the
corresponding labels, as well as the motion data given by the motion capture system. Then the same feature extraction part
is done, and the output features with labels are used to train the classifier. The motion data is used further for biological
interpretation. The lower part is the online pipeline, which starts from the EEG data stream and extracts the tangent space
projected covariance matrix as features for the classification. Some post-processing follows to ensure the rightness of the
classification results. Finally, the robot moves to the classified object and hands it over to the human.
                </p>
            </div>
        </div>
<br>
</body>
</html>
